#!/bin/bash

# getopts variables
OPTIND=1         # Reset in case getopts has been used previously in the shell.

show_help() {
cat << EOF
Usage: ${0##*/} <your-script> [-c executor_cores] [-d driver_memory] [-e executor_memory] [-h] [-j jars] [-m master] [-n num_executors]
    -c executor_cores       Number of cores to use for the driver process
    -d driver_memory        Amount of memory to use for the driver process
    -e executor_memory      Amount of memory to use per executor process
    -h                      display this help and exit
    -j jars                 any jars specified by this option will be automatically transferred to the cluster
    -m master               The cluster manager to connect to
    -p mode                 The cluster manager to connect to
    -n num_executors        number of spark executors to use for the job
EOF
}

BS_ROOT_DIR=/share/apps
BS_JAR_DIR=$BS_ROOT_DIR/lib/biospark/java
BS_SCRIPT_DIR=$BS_ROOT_DIR/lib/biospark/pyspark

# Initialize argument variables with default values:
executor_cores=1
driver_memory="2g"
executor_memory="2g"
jars=$BS_JAR_DIR/robertslab-hadoop.jar
master="yarn-client"
deploy_mode="client"
num_executors=4

while getopts "c:d:e:hj:m:p:n:" opt; do
    case "$opt" in
    c)  executor_cores=$OPTARG
        ;;
    d)  driver_memory=$OPTARG
        ;;
    e)  executor_memory=$OPTARG
        ;;
    h)  show_help
        exit 0
        ;;
    j)  jars=$BS_JAR_DIR/robertslab-hadoop.jar:$OPTARG
        ;;
    m)  master=$OPTARG
        ;;
    p)  deploy_mode=$OPTARG
        ;;
    n)  num_executors=$OPTARG
        ;;
    esac
done

shift $((OPTIND-1))

[ "$1" = "--" ] && shift

SCRIPT_NAME=$1
shift
if [ ! -f $SCRIPT_NAME ] && [ -f $BS_SCRIPT_DIR/$SCRIPT_NAME ]; then
    SCRIPT_NAME=$BS_SCRIPT_DIR/$SCRIPT_NAME
fi

DIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
echo "master=$master, num-executors=$num_executors, driver-memory=$driver_memory, executor-memory=$executor_memory, executor-cores=$executor_cores, jars=$jars, script-name=$SCRIPT_NAME, script_args=$@"

# Add check for spark version to know whetehr to use deploy mode or not.

# run spark submit with the specificed arguments
BS_JOB_TOTAL_CORES=$((num_executors*executor_cores)) spark-submit --master $master --num-executors $num_executors --driver-memory $driver_memory --executor-memory $executor_memory --executor-cores $executor_cores --jars $jars $SCRIPT_NAME "$@"
